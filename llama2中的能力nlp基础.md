# 总览

## RoPE
[Transformer升级之路：2、博采众长的旋转式位置编码 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8265)

## Attention
- k-v cache
  - [大模型推理性能优化之KV Cache解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/630832593)
- 分组查询注意力（Grouped-Query Attention，GQA），7b和13b模型并没有增加GQA，Llama2新加入
- rotary embeddings
  - [复数参考资料：复数基础与二维空间旋转 - 何雨龙 - 博客园 (cnblogs.com)/让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces (kexue.fm)](https://www.cnblogs.com/noluye/p/11964513.html)
## FFN
- SwiGLU激活函数：在前馈神经网络（FFN）使用SwiGLU 激活函数替换了Transformer中的 ReLU 激活函数来提升性能
- RMSNorm

参考资料
- https://www.mlpod.com/494.html
- [【llm大语言模型】一文看懂llama2(原理,模型,训练) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/651248009)
- 解析论文：https://zhuanlan.zhihu.com/p/644671690
                    
