# Peft
## LoRA
LoRA（Low Rank Adaptation）[1]，顾名思义，LoRA的核心思想是基于低秩的适配器进行优化。

[ LoRA’s](https://arxiv.org/abs/2106.09685) approach is to represent the **weight updates with two smaller matrices** (called **update matrices**) through **low-rank decomposition**。可以训练这些新矩阵以适应新数据，同时保持较低的变化总数。原始的权重矩阵被冻结了保持不变。最后结合原始和训练的权重。
LoRA训练起来smaller和faster，但是在推理时可能遇见时延问题因为需要分布加载基础模型和LoRA模型。To eliminate latency, use the **merge_and_unload()** function to merge the adapter weights with the base model which allows you to effectively **use the newly merged model as a standalone model.**
![Alt text](images/image_lora.png)
This works because during training, the smaller weight matrices (A and B in the diagram above) are separate. But once training is complete, the weights can actually be merged into a new weight matrix that is identical.

[因为自我认知训练涉及到知识编辑, 建议对MLP加lora_target_modules. 你可以通过指定--lora_target_modules ALL在所有的linear层(包括qkvo以及mlp)加lora. 这通常是效果最好的.](https://www.cnblogs.com/ting1/p/18219707)


那么什么是秩呢？矩阵的秩（rank）分为行秩和列秩，**行秩指的是矩阵的线性无关的行的个数，列秩同理**。因为一个矩阵的行秩和列秩总是相等的，因此它们统一被叫做矩阵的秩。在机器学习中，我们通常使用一个矩阵来表示一个全连接层，但是这个**全连接层往往是过参数化的**，这意味着我们可以通过计算这个矩阵的秩来确定哪些特征是重要和相关的。例如在主成分分析（PCA）和奇异值分解（SVD）中，我们通过一个较低维度的表示来近似表示一个高维矩阵或数据集（图1）。换句话说，我们试图找到原始特征空间（或矩阵）中少数维度的（线性）组合，能够捕捉数据集中大部分的信息。

低秩矩阵分解，这是一种特别有效的矩阵分解方法，用于发现数据中的低维结构。低秩矩阵分解的核心思想是将一个大矩阵分解为两个或多个更小、更简单的矩阵的乘积，这些小矩阵通常具有更低的秩。

LoRA 的思想很简单:

- 在原始 PLM (Pre-trained Language Model) 旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的intrinsic rank。
- 训练的时候固定 PLM 的参数，只训练降维矩阵 A 与升维矩阵 B 。而模型的输入输出维度不变，输出时将 BA 与 PLM 的参数叠加。
- 用随机高斯分布初始化 A ，用 0 矩阵初始化 B ，保证训练的开始此旁路矩阵依然是 0 矩阵。

1. 为什么A要使用高斯初始化，而B使用0初始化呢？
   LoRA并没有要求必须这么初始化，但是必须A或者B至少有一个是0，目的是为了当数据初次经过模型时，因为A或者B有一个是0，所以它的结果和原始的模型是一致的。但是苏剑林老师也认为可以使用两个非0矩阵做初始化，但是为了保持一致性，需要事先将预训练的权值减去初始化的权值。
2. 我们知道训练模型所消耗的显存来源包括模型参数、模型梯度、模型激活值、优化器状态四部份，LoRA通过低秩分解降低了模型参数量，那么梯度和优化器状态也会随之降低，因此节省的显存是很明显的。那它能否节省计算量呢？像Adapter、P-Tuning等很多参数高效的微调技巧，它们能够通过只微调很少的参数来达到接近全量参数微调的效果。然而，这些技巧通常只是“参数高效”而并非“训练高效”，因为它们依旧需要在整个模型中反向传播来获得少部分可训练参数的梯度，说白了，就是可训练的参数确实是少了很多，但是训练速度并没有明显提升。
在冻结原始参数 W 的情况下，LoRA（Low-Rank Adaptation）​前向传播仍需计算 xW，计算量不变；​在反向传播时能部分节省计算量​​，但节省的程度有限，具体取决于实现方式和框架优化。

尝试方法：

全参数SFT+LoRA微调模式：尝试了将全参数SFT与LoRA进行结合，具体微调的方式：前10%-30% step 采用全参数SFT的方式，后面的step采用LoRA的方式，比单纯的LoRA要更加稳定，比全部采用全量参数SFT更加节省资源。该方式动机，通常来讲，大模型微调的时候，前面step中，模型参数变化最快，loss也是下降的最快，后面step模型参数变化幅度越来越小，loss变化幅度变小，逐渐收敛。因此，可以在微调的最开始step采用全参数SFT，让模型能够尽快的学习到指令，后面采用LoRA的方式，让模型能够更好的遵循指令。全参数SFT与LoRA 训练step配比，可以依据自己的tokens来定。

## QLoRA（Quantized LoRA）​
### 核心思想​​：
​​量化 + LoRA​​：将预训练模型权重量化为 ​​4-bit​​ 或 ​​8-bit​​，同时结合低秩适配器（LoRA）进行微调，显著降低显存占用。

#### ​关键技术​​：
​​4-bit NormalFloat (NF4) 量化​​：非均匀量化策略，最小化信息损失。
​​动态反量化​​：前向/反向传播时临时恢复为高精度（FP16/FP32）计算。
​​分页优化器​​：避免梯度计算时的显存溢出。

QLoRA 和 LoRA 的核心区别确实在于 ​​是否引入预训练模型的权重量化​​，但这一改动带来的影响远不止“简单压缩权重”，而是涉及 ​​训练机制、显存优化、计算效率​​ 等多方面的差异。

| 特性         | LoRA                | QLoRA                             |
|------------|-------------------|-----------------------------------|
| 权重存储格式   | 原始精度（FP16/FP32） | 4-bit 量化（如NF4）               |
| 训练时显存占用 | 较高（需存储完整精度权重） | 极低（量化权重 + 反量化计算）       |
| 前向/反向传播机制 | 直接使用原始权重       | 动态反量化 → 计算 → 再量化         |
| 适配器设计   | 独立低秩矩阵（BA）     | 低秩矩阵 + 量化误差补偿             |

**关键差异**​​：
QLoRA 的量化不是静态的，而是在训练中 ​​动态反量化​​（每次前向/反向传播时临时恢复为高精度计算），因此能保持较高的数值精度。
#### ​​动态反量化（Dynamic Dequantization）​​ 
QLoRA 中的 ​​动态反量化（Dynamic Dequantization）​​ 是其核心创新之一，它解决了低比特量化（如4-bit）在训练过程中的数值精度损失问题。

​​1. 静态量化 vs 动态反量化​​
​​(1) 静态量化（传统方法）​​
​​操作​​：将预训练模型权重永久转换为低比特（如4-bit），训练和推理均直接使用量化后的权重。
​​问题​​：
训练时梯度计算在低精度下进行，数值误差累积导致模型性能下降。
无法恢复高精度信息，微调效果差。

​​(2) QLoRA的动态反量化​​
​​核心思想​​：
​​仅在需要计算时（前向/反向传播）将权重临时反量化为高精度（FP16/FP32），计算完成后再丢弃高精度值，保留原始量化权重。​​
​​优势​​：
显存中始终存储低比特权重，节省显存。
计算时使用高精度，减少数值误差。

#### 2. 动态反量化的具体流程

以 4-bit NormalFloat (NF4) 量化为例：

#### 步骤1：权重量化（存储阶段）
- 预训练权重矩阵 \( W \) 被量化为 4-bit：
  \[
  W_{4bit} = \text{Quantize}(W_{FP16}, \text{NF4})
  \]
  - Quantize 操作：将 FP16 值映射到 4-bit 的离散值（NF4 分布优化过，更适合神经网络权重）。

#### 步骤2：动态反量化（计算阶段）
- 前向传播时：
1. 从显存加载 $W_{4bit}$。
2. 临时反量化为 FP16：
3. $$
     W_{FP16} = \text{Dequantize}(W_{4bit})
   $$
  4. 计算输出：
     $$
     h = W_{FP16}x + BAx
     $$
  5. 丢弃 $W_{FP16}$，仅保留 $W_{4bit}$ 在显存中。

- 反向传播：
  1. 再次动态反量化 $W_{4bit}$ 为 $W_{FP16}$。
  2. 计算梯度 $\nabla W_{FP16}$ 并更新低秩矩阵 \( B \) 和 \( A \)。

#### 常见问题解答
​Q1：动态反量化会不会增加计算时间？​​

会，但显存节省允许更大的batch size，部分抵消时间开销。实际训练速度约为LoRA的61%（即慢39%）。
​​Q2：为何不直接全部用4-bit计算？​​

低比特算术（如4-bit矩阵乘）缺乏硬件支持，且梯度计算误差会导致训练失败。
​​Q3：推理时还需要动态反量化吗？​​

不需要。训练完成后可将适配器 BA 合并到量化权重中，推理时直接使用4-bit权重。
### ​优点​​：
​​显存极致优化​​：7B模型微调仅需5-6GB显存（如RTX 3090即可运行）。
​​性能接近全参数微调​​：量化误差通过LoRA补偿，任务性能损失小。
​​零推理开销​​：训练后合并量化权重与适配器，推理无需额外计算。
### ​缺点​​：
​​训练时间增加​​：量化/反量化操作增加约39%训练时间。
​​量化精度损失​​：极端低精度（如4-bit）可能影响复杂任务表现。

### 常见误解澄清​​
​​误解1：QLoRA只是“压缩版LoRA”​​
​​真相​​：QLoRA是 ​​量化+动态反量化+LoRA​​ 的协同设计，量化误差通过适配器微调补偿，而非简单压缩。

​​误解2：QLoRA一定会降低模型性能​​
​​真相​​：在合理配置下（如NF4量化+足够秩的适配器），QLoRA性能损失可忽略，且显存节省显著。

​​误解3：QLoRA和LoRA的适配器结构不同​​
​​真相​​：两者适配器（低秩矩阵BA）设计相同，区别仅在于QLoRA的适配器需补偿量化误差。

## AdaLoRA
### 核心思想​​：
​- ​动态调整秩​​：根据模块对任务的重要性，自适应分配秩 r。
重要模块分配更高秩（保留更多信息），次要模块降低秩（减少冗余）。
​- 奇异值裁剪​​：通过修剪不重要的奇异值，实现参数高效分配。
### 数学实现
1. 参数化增量矩阵：
   奇异值是矩阵 A 奇异值分解（SVD）的一部分，它描述了矩阵在不同维度上的缩放因子。
   AdaLoRA 将 ΔW 表示为奇异值分解（SVD）形式：
   $$
   \Delta W = P \Lambda Q, \quad P \in \mathbb{R}^{d \times r}, Q \in \mathbb{R}^{r \times k}, \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_r)
   $$
   - Λ 是对角矩阵，λ_i 为可训练奇异值。
   - P 和 Q 是正交矩阵（通过梯度更新后投影回正交空间保持稳定性）。

2. 重要性评分：
   对每个奇异值 λ_i，计算其重要性分数 s_i：
   $$
   s_i = \lambda_i^2 \cdot \|P_i\|_2^2 \cdot \|Q_i\|_2^2
   $$
   - P_i, Q_i 是 P 和 Q 的第 i 列行。
   - 分数 s_i 反映该奇异值对任务的贡献。

3. 动态调整秩：
   - 定期根据 s_i 排序，裁剪分数最低的奇异值（设为0）。
   - 重新分配秩 r 给剩余奇异值，优先保留高重要性部分。

训练过程：
1. 初始化 P, Λ, Q，设定初始秩 r。
2. 前向传播：$h = Wx + PAQx$。
3. 反向传播：更新 P, Λ, Q，并计算重要性分数 s_i。
4. 每隔 T 步裁剪和调整秩。
   
### ​直观示例​​：
假设微调一个 ​​多语言翻译模型​​：

​​LoRA​​：对所有语言的Attention层使用相同的秩 r=8，但某些语言对（如英-法）可能需要更高秩，而低资源语言（如冰岛语）可能只需 r=2。
​​AdaLoRA​​：自动为英-法分配 r=12，冰岛语分配 r=3，动态优化参数分配。

| 特性     | LoRA           | AdaLoRA                             |
|--------|--------------|-----------------------------------|
| 秩分配   | 固定秩 \(r\)   | 动态调整秩（各模块不同）         |
| 参数效率 | 静态低秩矩阵   | 基于重要性剪枝奇异值               |
| 计算开销 | 较低（固定结构） | 稍高（需计算/裁剪奇异值）         |
| 适用场景 | 各模块重要性均匀的任务 | 模块重要性差异大的复杂任务         |
| 实现复杂度 | 简单（仅 \(BA\) 矩阵） | 较复杂（SVD + 正交约束）         |


### 优缺点总结​：
​​AdaLoRA 的优势​​：
​1. ​参数更高效​​：避免冗余低秩矩阵，在相同总参数量下性能更高。
论文显示：在GLUE任务上，AdaLoRA用 ​​30% 参数​​ 达到LoRA（全秩）的性能。
​2. ​任务适应性​​：动态调整适合复杂任务（如多任务学习、多语言场景）。
​3. ​避免过拟合​​：通过剪枝不重要方向，类似隐式正则化。

​​AdaLoRA 的劣势​​：
​1. ​计算开销​​：SVD分解和正交投影增加训练时间（约10%~20%）。
​​2. 超参数敏感​​：需调整初始秩、裁剪频率 T、重要性阈值等。
​​3. 实现复杂​​：需处理梯度更新后的正交约束（如Gram-Schmidt过程）。

​​LoRA 的适用场景​​：
模块重要性差异小的任务（如单语言文本生成）。
资源受限，需快速部署的场景。
## LoRA-GA
LoRA-GA（​​LoRA with Gradient Accumulation​​）是一种结合了 ​​低秩自适应（LoRA）​​ 和 ​​梯度累积（Gradient Accumulation）​。

(1) 背景与问题​​
​​传统LoRA的局限​​：
LoRA通过低秩矩阵B和A微调大模型，显著减少参数量，但仍需存储完整模型权重（FP16/FP32），显存占用较高。
在显存不足时（如消费级GPU），无法支持较大batch size，影响训练稳定性。
​​梯度累积的作用​​：
通过多次前向传播累积梯度，再统一更新参数，等效增大batch size，但传统梯度累积需存储中间激活值，显存压力更大。
​​(2) LoRA-GA的创新​​
​​动态梯度累积 + LoRA​​：
仅在 ​​低秩矩阵（BA）​​ 上累积梯度，避免存储完整模型的中间激活。
结合量化技术（如4-bit权重），进一步降低显存占用

## DoRA（Weight-Decomposed Low-Rank Adaptation）
DoRA（​​权重分解的低秩自适应​​）是LoRA的改进版，通过​​将预训练权重分解为“幅度（magnitude）”和“方向（direction）”两部分​​，并仅对方向部分做低秩更新，显著提升微调效果。其核心创新点包括：

​​权重分解​​：将原始权重 W 分解为幅度 m（标量）和方向 V（单位向量）。
​​方向低秩更新​​：仅对方向部分 V 应用LoRA，幅度 m 保持可训练。
​​增强模型表达能力​​：分离幅度和方向的调整，避免LoRA中方向与幅度耦合导致的优化受限问题。

- 原始权重分解：
  $$
  W = m \cdot \frac{V}{\|V\|_2}, \quad \text{其中 } m \in \mathbb{R}, V \in \mathbb{R}^{d \times k}
  $$
  - \( m \)：控制权重的幅度（缩放因子）。
  - \( V \)：方向矩阵（单位化后决定权重方向）。
∥V∥ 表示矩阵（或向量）V 的 ​​L2范数​​（也称为欧几里得范数）。表示矩阵（或向量）所有元素的平方和的平方根，即“长度”，用于将方向矩阵 V 归一化为单位长度，从而实现方向与幅度的解耦

- DoRA 的增量更新：
  $$
  \Delta V = BA, \quad \text{LoRA 仅作用于方向 } V
  $$
  - 训练时更新 \( m \) 和低秩矩阵 \( B \)、\( A \)，冻结原始 \( V \)。

- 最终权重：
  $$
  W_{\text{DoRA}} = m \cdot \frac{V + BA}{\|V + BA\|_2}
  $$
  注：分母的归一化保证方向调整后仍为单位向量。
## LoRA-pro
