# 概述
[图文多模态”，包括“图”和“文”的表征方法，然后是图文表征的融合方法。](https://zhuanlan.zhihu.com/p/684472814)
- 文本表征
- 视觉表征从发展上可以分为卷积神经网络（CNN）和Vision Transformer（VIT）两大脉络，二者分别都有各自的表征、预训练以及多模态对齐的发展过程。总的来说可以分为两个部分问题
  - 一是如何合理**建模视觉输入特征**，比如cnn是一种建模模型
  - 二是如何通过**预训练手段进行充分学习表征**，这两点是基于视觉完成具体算法任务的基础；
- 视觉与自然语言的对齐（Visul Language Alignment）或融合：目的是将视觉和自然语言建模到同一表征空间并进行融合，实现自然语言和视觉语义的互通，这点同样离不开预训练这一过程。模态对齐是处理多模态问题的基础，也是现在流行的多模态大模型技术前提。
- 
![Alt text](image.png)

如何对视觉特征进行有效编码，得到和文本一样的Token Embedding序列作为模型输入？这一问题的解法在CNN为主的时期有两种主要方式，如图5：

Region Feature Base：先通过基于CNN的目标检测模型（Fast R-CNN等），识别图像中的关键物体区域集合（ROI，Region Of Interest），并提取区域的表征向量，作为Transformer模型的视觉输入Embedding序列。这么做的动机是，每个ROI区域，都有明确的语义表达（人、建筑、物品等），方便后续和文本特征的对齐。比较有代表性的工作如LXMERT、VL-BERT和UNITER等；
Grid Feature Base：区域特征方法虽然看上去合理，但是依赖前置的目标检测模型，整体链路较重。因此也有工作探索，不经过区域检测，直接使用CNN网络提取深层的像素特征作为交互模型输入，同样取得了一些成果。比较有代表性的工作如Pixel-Bert等。


CNN体系下的多模态融合和预训练，视觉和自然语言的跨模态对齐和融合有两种表现形式：一种是双塔结构，多模态分别表征，通过对比学习机制实现视觉和文本在同一空间中的距离度量；另一种是视觉表征和文本表征通过交互型网络结构融合成多模态表征，进而完成下游任务应用。

# deepseek-VL
## 模型架构
### 1. Hybrid Vision Encoder
#### clip模型局限性
>Vision encoders in the CLIP family, including SigLIP, are primarily designed for semantic visual representations but are challenged by ambiguous encoding, resulting in visually distinct images being encoded as similar due to what is referred to as "CLIP-blind pairs" 

- CLIP-blind pairs: CLIP模型确实在某些情况下可能会将视觉上不同的图像编码为相似的表示，这主要是因为CLIP的训练目标是学习语义上相关的图像和文本对。这意味着CLIP更关注图像的语义信息，而不是细粒度的视觉差异。因此，如果两个视觉上不同的图像在语义上非常相似，CLIP可能会将它们编码为相似的表示，这可以被描述为“CLIP-blind pairs”。
- 语义视觉表示: CLIP的设计目标确实是为图像和文本学习一种语义相关的表示，它通过大规模的图像-文本对进行训练，使得在语义层面上相似的图像和文本能够靠近彼此。
- “ambiguous encoding”: CLIP的“模糊编码”问题实际上可以理解为，CLIP在处理一些视觉信息时，可能无法精确区分某些细微的视觉差异，尤其是在这些差异对于语义并不重要的情况下。这并非CLIP模型的设计缺陷，而是其侧重于语义表达的自然结果。

#### hybrid vision encoder的组成
>we additionally utilize a vision-only encoder based on the SAM-B, a pre-trained ViTDet (Li et al., 2022) image encoder to process low-level features, which accepts high-resolution 1024 x 1024 image inputs. In addition to the SAM-B encoder, we retain the SigLIP-L vision encoder with low-resolution 384 x 384 image inputs. Consequently,**our hybrid vision encoder combines the SAM-B and SigLIP-L encoders,** efficiently encoding high-resolution 1024 x 1024 images while preserving both semantic and detailed information.Specifically, a high-resolution SAM-B vision encoder first resizes the image into 1024 x 1024 and results in a 64 x 64 x 256 feature map.

论文的描述介绍了一种hybrid vision encoder的建模方法，该方法结合了两种不同的视觉编码器——SAM-B（基于ViTDet的图像编码器）和SigLIP-L，以达到在高分辨率图像中既能保留语义信息又能捕捉细节信息的目的。
1. 使用SAM-B encoder处理高分辨率图像：
- 这个编码器基于ViTDet，是一个预训练的图像编码器。
- 它接受高分辨率的1024 x 1024图像作为输入，并将图像处理成一个64 x 64 x 256的特征图。
- SAM-B主要用于捕捉低层次的细节特征，因为高分辨率输入使其能够捕捉到更多的细节信息。
2. 保留SigLIP-L encoder处理低分辨率图像：
- SigLIP-L接受低分辨率的384 x 384图像输入。
- 这个编码器主要用于提取语义信息，因此低分辨率的图像输入足以捕捉图像中的语义特征。
3. hybrid vision encoder设计：
- 将上述两种编码器结合，形成一个混合的视觉编码器。
- 通过这种组合，模型能够有效地编码高分辨率的1024 x 1024图像，同时保留语义信息和图像的细节特征。
总结来说，这种建模做法利用了不同编码器在处理不同分辨率图像时的优势，旨在通过融合低层次细节信息和高层次语义信息，提升视觉编码的精度和有效性。

#### 高分辨率和低分辨率特征怎么融合
1. 高分辨率特征图处理：
- SAM-B编码器生成了一个大小为64 x 64 x 256的高分辨率特征图。
- VL adaptor首先将该特征图通过插值处理为96 x 96 x 256的大小，以增加特征图的分辨率。
- 然后，VLadaptor通过两个步幅为2的卷积层，将特征图进一步缩小到24 x 24 x 1024的大小。
- 最后，将24 x 24 x 1024的特征图重塑为576 x 1024的形状。
2. 低分辨率特征图处理：
- SigLIP-L编码器生成一个大小为576 x 1024的低分辨率特征图。
- 该特征图直接与高分辨率特征图拼接在一起，形成576个维度为2048的视觉标记（tokens）。
3. 视觉标记处理：
- 这些视觉标记既包含了高分辨率的详细信息，也保留了低分辨率的语义信息。
- 这些视觉标记通过GeLU激活函数进行非线性处理。
- 然后，它们通过一个嵌入层（embedding layer），用于与语言模型建立联系。

### 2. Vision-Language Adaptor
1. 使用单层MLP分别处理高分辨率和低分辨率特征：
- 首先，视觉编码器产生的高分辨率特征和低分辨率特征分别通过独立的单层多层感知器（MLP）进行处理。
- 这些单层MLP的作用是对高分辨率和低分辨率的特征进行初步的转换和调节，以适应后续的特征融合。
2. 特征的维度拼接：
- 在经过单层MLP处理之后，高分辨率和低分辨率的特征会在它们的维度上进行拼接。
- 这种拼接将两种不同分辨率的特征融合在一起，形成一个统一的特征表示。
3. 通过另一层MLP转换为LLM的输入空间：
- 拼接后的特征通过一个多层感知器（MLP）进一步转换。
- 这个MLP的作用是将融合后的视觉特征映射到大规模语言模型（LLM）的输入空间，使得这些视觉特征能够被LLM理解和处理。

简单示例：

```python
import torch
import torch.nn as nn

# 假设高分辨率和低分辨率特征的维度分别为 [batch_size, 256] 和 [batch_size, 512]
class VisionLanguageAdaptor(nn.Module):
    def __init__(self, high_res_dim=256, low_res_dim=512, mlp_hidden_dim=1024, output_dim=768):
        super(VisionLanguageAdaptor, self).__init__()
        
        # 处理高分辨率特征的单层MLP
        self.high_res_mlp = nn.Linear(high_res_dim, mlp_hidden_dim)
        
        # 处理低分辨率特征的单层MLP
        self.low_res_mlp = nn.Linear(low_res_dim, mlp_hidden_dim)
        
        # 拼接后通过的MLP
        self.output_mlp = nn.Linear(mlp_hidden_dim * 2, output_dim)
        
        # 激活函数
        self.activation = nn.GELU()

    def forward(self, high_res_features, low_res_features):
        # 处理高分辨率特征
        high_res_out = self.activation(self.high_res_mlp(high_res_features))
        
        # 处理低分辨率特征
        low_res_out = self.activation(self.low_res_mlp(low_res_features))
        
        # 拼接特征
        concatenated_features = torch.cat((high_res_out, low_res_out), dim=-1)
        
        # 转换为语言模型的输入空间
        output = self.activation(self.output_mlp(concatenated_features))
        
        return output

# 示例输入
batch_size = 8
high_res_features = torch.randn(batch_size, 256)
low_res_features = torch.randn(batch_size, 512)

# 实例化Vision-Language Adaptor
adaptor = VisionLanguageAdaptor()

# 前向传播
output = adaptor(high_res_features, low_res_features)

print(output.shape)  # 输出的形状应为 [batch_size, output_dim]，即 [8, 768]
```

### 3. Language Model
1. 基础模型设计：
基于DeepSeek LLM: 这个语言模型是基于DeepSeek LLM（DeepSeek-AI, 2024）构建的。DeepSeek微观设计与LLaMA模型（Touvron等, 2023a,b）的设计非常相似。
2. 具体的模型结构：
- Pre-Norm结构: 模型采用了Pre-Norm结构，在每个Transformer块中先对输入进行标准化再进行计算。
- RMSNorm: 使用RMSNorm（Zhang and Sennrich, 2019）作为标准化函数，这是一种与LayerNorm类似的归一化方法，但计算更高效。
- SwiGLU激活函数: 在Feed-Forward Network (FFN)中使用了SwiGLU（Shazeer, 2020）作为激活函数。这种激活函数被证明在多种模型中能够提供更好的性能。
- 中间层维度: FFN的中间层维度被设定为 8/3 × 𝑑_model，即模型维度的8/3倍。
3. 位置编码：
- 旋转嵌入（Rotary Embedding）: 模型使用旋转嵌入（Rotary Embedding）来进行位置编码，这是一种在自注意力机制中应用的相对位置编码方法，能够更好地捕捉序列中的位置信息。
4. Tokenizer：
- 与DeepSeek-LLM相同: 这个语言模型使用了与DeepSeek-LLM相同的分词器（tokenizer），确保文本输入的一致性和兼容性。

## 模型Training Pipelines
### 1. vision-language adaptor warmup
**这一步的主要目标是通过embedding空间在视觉和语言元素之间建立概念上的联系**，从而使大型语言模型（LLM）能够全面理解图像中所描绘的实体。这种训练方法与之前的研究方法（如LLaVA和Instruct-BLIP）一致，这些方法也采用了冻结主要模型（视觉编码器和LLM）并仅训练适配器的策略。这种策略能够有效地利用预训练模型的优势，同时通过训练适配器来增强模型在多模态任务中的表现。
1. 冻结模型：
- 冻结 vision encoder:  vision encoder在训练过程中保持冻结，即它的参数不发生变化。这意味着 vision encoder只用于提取视觉特征，而不会被进一步训练。
- 冻结大型语言模型（LLM）: 同样，LLM在训练过程中也保持冻结，语言模型的参数不发生变化。这意味着LLM在训练过程中不会更新，它只接收由vision-language adaptor生成的输入。
- 仅训练视觉-语言适配器vision-language adaptor: 唯一允许训练的部分是视觉-语言适配器（VL Adaptor）。这个适配器的参数会根据训练数据进行调整，以便在视觉特征和语言特征之间建立有效的联系。
2. 使用的数据集：
- 图像-文本配对: 使用了从ShareGPT4V获取的125万张图像-文本配对数据。这些配对数据可能包括图像和对应的描述性文本，用于帮助模型学习如何将视觉信息映射到语言表示中。
- 文档OCR渲染配对: 使用了250万对文档OCR渲染数据。这些数据可能涉及文档图像及其OCR（光学字符识别）生成的文本对，用于进一步增强模型在视觉和文本之间建立联系的能力。
- 
3. VL-adaptor的局限性和训练策略的调整：

vision-language adaptors（如一个两层的MLP）相比于大型语言模型（LLM）具有显著较小的参数容量。由于模型容量的限制，这个阶段的视觉-语言适配器在训练过程中能够学习的能力受到限制，无法充分利用更复杂或更大规模的数据集。

数据规模扩展data scaling的实验：
为了探讨数据扩展data scaling在这个阶段是否有效，作者进行了一个简单的实验。实验结果表明，在这个阶段扩展数据规模并没有带来性能提升，甚至可能导致性能下降。这表明，在视觉-语言适配器训练阶段，增加数据量并不能有效弥补适配器模型容量的不足。

训练策略的调整：
- 解冻大型语言模型（LLM）：由于在第一个阶段仅训练视觉-语言适配器的效果有限，研究者决定在第二阶段解冻大型语言模型（LLM）。
- 研究更高效的视觉-语言预训练方法：解冻LLM后，研究者探讨更高效的视觉-语言预训练方法，以充分利用LLM的能力。这种方法可以包括联合训练视觉编码器、视觉-语言适配器和语言模型，或者引入其他复杂的训练策略来提高多模态学习的效果。

**总结：**
**初始阶段，仅通过小容量的视觉-语言适配器进行训练，由于模型容量的限制，即使增加数据量也难以获得明显的性能提升。
为了克服这种限制，研究者在第二阶段解冻LLM，使其可以与视觉-语言适配器共同学习和调整，从而探索更高效的预训练方法，以提升整体模型的多模态理解能力。**

### 2. joint vision-language pretraining

### 3. supervised fine-tuning
